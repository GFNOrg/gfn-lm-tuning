{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "source": [
    "# Random Number Generation Experiments\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "![RNG diagram](../rng.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook sets up experiments comparing different methods for training language models to generate random numbers from specified distributions.\n",
    "\n",
    "We will focus on sampling numbers from various distributions.\n",
    "\n",
    "The models we will compare are:\n",
    "\n",
    "- GFN-fine-tuned LM: Fine-tuned via generative flow networks\n",
    "- Likelihood-trained LM: Supervised-fine-tuned LM\n",
    "- RL-tuned LM: Fine-tuned via reinforcement learning (PPO)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several axes of experimentation:\n",
    "\n",
    "- vary the distribution\n",
    "  - discrete: uniform, Poisson, Binomial, Geometric, etc\n",
    "  - continuous: uniform, Gaussian, exponential, etc\n",
    "- vary the hyperparameters of the distribution (in the context)\n",
    "  - Uniform: between 0 and `n_max`\n",
    "  - Poisson: `lambda` between `Î»_min` and `Î»_max`\n",
    "  - etc\n",
    "- vary the prompt\n",
    "  - 'Randomly generate (uniformly) one single random integer between 0 and {num_test}, and then stop: '\n",
    "  - 'Randomly generate (uniformly) one single random integer in the interval [0, {num_test}]: '\n",
    "  - 'Here is one single random integer sampled uniformly between 0 and {num_test}: '\n",
    "  - \"The following is a random integer drawn uniformly between 0 and {num_test}: \"\n",
    "  - etc\n",
    "- vary the model\n",
    "  - GFN-LM\n",
    "  - PPO\n",
    "  - MLE (SFT)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import hydra\n",
    "from hydra.experimental import initialize, compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xq77AgKWJM-N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-28 22:44:38,741] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# from dataclasses import dataclass, field\n",
    "from trl import (\n",
    "    AutoModelForCausalLMWithValueHead,\n",
    "    PPOConfig,\n",
    "    PPOTrainer,\n",
    "    create_reference_model,\n",
    "    set_seed,\n",
    ")\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# # Remove the matplotlib cache\n",
    "# shutil.rmtree(matplotlib.get_cachedir())\n",
    "\n",
    "fonts = matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext=\"ttf\")\n",
    "\n",
    "# print the names of all fonts\n",
    "font_names = [matplotlib.font_manager.get_font(x).family_name for x in fonts]\n",
    "print(font_names)\n",
    "\n",
    "fonts = [f.name for f in matplotlib.font_manager.fontManager.ttflist]\n",
    "print(fonts)\n",
    "print(\"Times New Roman\" in fonts)\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "matplotlib.rc(\"font\", family=\"Times New Roman\")\n",
    "\n",
    "print(matplotlib.get_configdir())\n",
    "print(matplotlib.get_cachedir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 28 22:44:43 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              62W / 400W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "with torch.no_grad():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the pretrained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Hydra\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"multiobjective-lm/rng/configs\")\n",
    "cfg = compose(config_name=\"config\")\n",
    "\n",
    "warmup_steps = cfg.hparams.warmup_steps\n",
    "max_len = cfg.hparams.max_len\n",
    "min_len = cfg.hparams.min_len\n",
    "eval_interval = cfg.hparams.eval_interval\n",
    "log_interval = cfg.hparams.log_interval\n",
    "model_to_use = cfg.hparams.model_to_use\n",
    "seed = cfg.hparams.seed\n",
    "save_dir = cfg.hparams.save_dir\n",
    "lr = cfg.hparams.PPO.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "set_seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA, Optimizer, PPO Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "current_device = Accelerator().local_process_index\n",
    "print(current_device)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=cfg.lora.r,\n",
    "    lora_alpha=cfg.lora.lora_alpha,\n",
    "    target_modules=[\"k_proj\", \"v_proj\"] if model_to_use == \"gpt-j\" else [\"c_attn\"],\n",
    "    lora_dropout=cfg.lora.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "inference_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    \"nlpcloud/instruct-gpt-j-fp16\" if model_to_use == \"gpt-j\" else \"gpt2\",\n",
    "    # load_in_8bit=True,\n",
    "    device_map={\"\": current_device},\n",
    "    peft_config=lora_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"nlpcloud/instruct-gpt-j-fp16\" if model_to_use == \"gpt-j\" else \"gpt2\"\n",
    ")\n",
    "\n",
    "opt = torch.optim.AdamW(\n",
    "    [{\"params\": inference_model.parameters(), \"lr\": lr}],\n",
    "    betas=(cfg.adamw.b1, cfg.adamw.b2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: No names found, cannot describe anything.\n"
     ]
    }
   ],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=model_to_use,\n",
    "    log_with=\"wandb\",\n",
    "    steps=cfg.hparams.PPO.steps,\n",
    "    learning_rate=cfg.hparams.PPO.learning_rate,\n",
    "    batch_size=cfg.hparams.PPO.batch_size,\n",
    "    ppo_epochs=cfg.hparams.PPO.ppo_epochs,\n",
    "    gradient_accumulation_steps=cfg.hparams.PPO.gradient_accumulation_steps,\n",
    "    early_stopping=cfg.hparams.PPO.early_stopping,\n",
    "    target_kl=cfg.hparams.PPO.target_kl,\n",
    "    init_kl_coef=cfg.hparams.PPO.init_kl_coef,\n",
    "    adap_kl_ctrl=cfg.hparams.PPO.adap_kl_ctrl,\n",
    "    mini_batch_size=cfg.hparams.PPO.mini_batch_size,\n",
    "    optimize_cuda_cache=cfg.hparams.PPO.optimize_cuda_cache,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "# WARNING!! To avoid the bug: https://github.com/huggingface/trl/issues/648\n",
    "# make sure that the following assertion is true!\n",
    "assert config.mini_batch_size * config.gradient_accumulation_steps <= config.batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...: 2048it [00:00, 5412.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from rng.rng_dataset import get_tensors_from_dataframe\n",
    "\n",
    "df_train = pd.read_csv(cfg.file_name.train)\n",
    "\n",
    "input_ids, target_ids = get_tensors_from_dataframe(df_train, tokenizer, method=\"PPO\")\n",
    "train_dataset = TensorDataset(input_ids, target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_dataset) * cfg.hparams.PPO.ppo_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rng.rng_utils import number_from_generated_text\n",
    "\n",
    "ref_model = create_reference_model(inference_model, num_shared_layers=20)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "eos_string = tokenizer.decode(tokenizer.eos_token_id)\n",
    "\n",
    "\n",
    "def collator(batch):\n",
    "    input_ids, target_ids = zip(*batch)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"query\": [s.replace(eos_string, \"\") for s in tokenizer.batch_decode(input_ids)],\n",
    "        \"target_ids\": target_ids,\n",
    "    }\n",
    "\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    inference_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=train_dataset,\n",
    "    data_collator=collator,\n",
    "    optimizer=opt,\n",
    "    # lr_scheduler=sched,\n",
    ")\n",
    "\n",
    "generation_kwargs = {\n",
    "    # \"min_length\": -1,\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "output_min_length = 2\n",
    "output_max_length = 128\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "model_save_path = os.path.join(save_dir, \"ppo_model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch, batch in tqdm(\n",
    "    enumerate(ppo_trainer.dataloader), total=len(ppo_trainer.dataloader)\n",
    "):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "    target_tensors = batch[\"target_ids\"]\n",
    "\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[query.shape[0] :])\n",
    "\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    # Compute reward\n",
    "    rewards = []\n",
    "    # mean_rewards = 0\n",
    "    for response, target in tqdm(zip(batch[\"response\"], target_tensors)):\n",
    "        if len(response) == 0:\n",
    "            # print(\"âŒ Empty response\")\n",
    "            scalar_reward = -16.0\n",
    "        else:\n",
    "            try:\n",
    "                generated_number = int(response.replace(eos_string, \"\").rstrip())\n",
    "                # print(\"âœ… Generated number:\", generated_number)\n",
    "                scalar_reward = 16.0 if 0 <= generated_number <= 100 else -8.0\n",
    "            except:\n",
    "                # print(\"âŒ Error decoding response:\", response)\n",
    "                scalar_reward = -8.0 * len(response)\n",
    "        rewards.append(torch.tensor(scalar_reward).cuda())\n",
    "        # mean_rewards += scalar_reward\n",
    "    # mean_rewards /= len(batch[\"response\"])\n",
    "\n",
    "    # Run PPO step\n",
    "    stats = ppo_trainer.step(list(query_tensors), response_tensors, rewards)\n",
    "    # Average reward\n",
    "    # print(\"ðŸ”¥ Average reward: \", mean_rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    # # Save model every 2 epochs\n",
    "    # if epoch % 2 == 0:\n",
    "    #     if ppo_trainer.accelerator.is_main_process:\n",
    "    #         ppo_trainer.save_pretrained(model_save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test generations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' the', ' number', ' is', ' 113', '.', '\\n', '<|endoftext|>']\n",
      "[' 4', '\\n', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "inference_model.eval()\n",
    "with torch.inference_mode():\n",
    "    prompt_test = \"Randomly generate (uniformly) one single random integer between 0 and 520, and then stop: \"\n",
    "    print(\n",
    "        [\n",
    "            tokenizer.decode(t)\n",
    "            for t in inference_model.generate(\n",
    "                **tokenizer(prompt_test, return_tensors=\"pt\").to(\"cuda\"),\n",
    "                max_new_tokens=30,\n",
    "                temperature=0\n",
    "            )[0][len(tokenizer.encode(prompt_test)) :]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt_test = (\n",
    "        \"Here is one single random integer sampled uniformly between 0 and 520: \"\n",
    "    )\n",
    "    print(\n",
    "        [\n",
    "            tokenizer.decode(t)\n",
    "            for t in inference_model.generate(\n",
    "                **tokenizer(prompt_test, return_tensors=\"pt\").to(\"cuda\"),\n",
    "                max_new_tokens=30,\n",
    "                temperature=0\n",
    "            )[0][len(tokenizer.encode(prompt_test)) :]\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_hparams = f\"{model_to_use}_batch_size_{cfg.hparams.PPO.batch_size}_mini_batch_size_{cfg.hparams.PPO.mini_batch_size}_steps_{cfg.hparams.PPO.steps}_learning_rate_{cfg.hparams.PPO.learning_rate}_ppo_epochs_{cfg.hparams.PPO.ppo_epochs}_gradient_accumulation_steps_{cfg.hparams.PPO.gradient_accumulation_steps}_target_kl_{cfg.hparams.PPO.target_kl}_init_kl_coef_{cfg.hparams.PPO.init_kl_coef}_seed_{seed}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory ckpts/rng-PPO_gpt-j_batch_size_8_mini_batch_size_1_steps_512_learning_rate_1.41e-05_ppo_epochs_1_gradient_accumulation_steps_8_target_kl_0.1_init_kl_coef_0.2_seed_42\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    print(f\"Creating directory {save_dir}\")\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "ckpt_name = f\"rng-PPO_{suffix_hparams}\"\n",
    "\n",
    "if not os.path.exists(f\"{save_dir}/{ckpt_name}\"):\n",
    "    print(f\"Creating directory {save_dir}/{ckpt_name}\")\n",
    "    os.makedirs(f\"{save_dir}/{ckpt_name}\")\n",
    "\n",
    "inference_model.save_pretrained(f\"{save_dir}/{ckpt_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_name = \"rng-PPO_gpt-j_bsz_8_grad_acc_1_lr_5e-05_warmup_steps_100_epochs_10_max_len_512_min_len_1_eval_interval_100_log_interval_10_seed_42\"\n",
    "# model_path = f\"{save_dir}/{ckpt_name}\"\n",
    "# inference_model = PeftModel.from_pretrained(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rng.rng_utils import get_distribution\n",
    "\n",
    "n_max = 100\n",
    "intro_prompt = f\"The following is a random integer drawn uniformly between 0 and \"\n",
    "prompt = f\"{intro_prompt}{n_max-1}: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rng.rng_plot import plot_distribution\n",
    "\n",
    "n_samples = 1000 * 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [55:43<00:00,  3.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# inference_model.base_model.enable_adapter_layers()\n",
    "\n",
    "dist_inference, number_of_NaNs_inference = get_distribution(\n",
    "    inference_model, tokenizer, prompt, num_samples=n_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490589 numbers, 21411 NaNs\n"
     ]
    }
   ],
   "source": [
    "number_of_numbers = len(dist_inference)\n",
    "\n",
    "print(f\"{number_of_numbers} numbers, {number_of_NaNs_inference} NaNs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## PPO-finetuned Model: Distribution of generated numbers"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_distribution(\n",
    "    dist_inference_modified,\n",
    "    n_max=n_max,\n",
    "    model_name=\"PPO-finetuned Model\",\n",
    "    color=\"goldenrod\",\n",
    "    number_of_NaNs=number_of_NaNs_inference,\n",
    "    xlims=(-5, n_max + 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"plots\"):\n",
    "    os.makedirs(\"plots\")\n",
    "\n",
    "df_inference = pd.DataFrame(dist_inference, columns=[\"Generated Numbers\"])\n",
    "df_inference.to_csv(\n",
    "    f\"plots/PPO_nb-numbers_{number_of_numbers}_nb-NaNs_{number_of_NaNs_inference}_{suffix_hparams}.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generated Numbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Generated Numbers\n",
       "0                 49\n",
       "1                 50\n",
       "2                 54\n",
       "3                 42\n",
       "4                 88"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inference.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
