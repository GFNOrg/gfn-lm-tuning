{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "source": [
    "# Random Number Generation Experiments\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "![RNG diagram](../rng.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook sets up experiments comparing different methods for training language models to generate random numbers from specified distributions.\n",
    "\n",
    "We will focus on sampling numbers from various distributions.\n",
    "\n",
    "The models we will compare are:\n",
    "\n",
    "- GFN-fine-tuned LM: Fine-tuned via generative flow networks\n",
    "- Likelihood-trained LM: Supervised-fine-tuned LM\n",
    "- RL-tuned LM: Fine-tuned via reinforcement learning (PPO)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several axes of experimentation:\n",
    "\n",
    "- vary the distribution\n",
    "  - discrete: uniform, Poisson, Binomial, Geometric, etc\n",
    "  - continuous: uniform, Gaussian, exponential, etc\n",
    "- vary the hyperparameters of the distribution (in the context)\n",
    "  - Uniform: between 0 and `n_max`\n",
    "  - Poisson: `lambda` between `Î»_min` and `Î»_max`\n",
    "  - etc\n",
    "- vary the prompt\n",
    "  - 'Randomly generate (uniformly) one single random integer between 0 and {num_test}, and then stop: '\n",
    "  - 'Randomly generate (uniformly) one single random integer in the interval [0, {num_test}]: '\n",
    "  - 'Here is one single random integer sampled uniformly between 0 and {num_test}: '\n",
    "  - \"The following is a random integer drawn uniformly between 0 and {num_test}: \"\n",
    "  - etc\n",
    "- vary the model\n",
    "  - GFN-LM\n",
    "  - PPO\n",
    "  - MLE (SFT)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import hydra\n",
    "from hydra.experimental import initialize, compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xq77AgKWJM-N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-11 06:27:45,432] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# from dataclasses import dataclass, field\n",
    "from trl import (\n",
    "    AutoModelForCausalLMWithValueHead,\n",
    "    PPOConfig,\n",
    "    PPOTrainer,\n",
    "    create_reference_model,\n",
    "    set_seed,\n",
    ")\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mila/y/younesse.kaddar/gfn-lm-tuning\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "    not os.path.exists(\"gfn-lm-tuning\")\n",
    "    and os.path.basename(os.getcwd()) != \"gfn-lm-tuning\"\n",
    "):\n",
    "    !git clone https://github.com/GFNOrg/gfn-lm-tuning\n",
    "    %cd gfn-lm-tuning\n",
    "elif os.path.basename(os.getcwd()) != \"gfn-lm-tuning\":\n",
    "    %cd gfn-lm-tuning\n",
    "else:\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DejaVu Sans', 'DejaVu Sans', 'Droid Sans Fallback', 'DejaVu Sans Mono', 'DejaVu Sans Mono', 'Times New Roman', 'DejaVu Serif', 'DejaVu Serif', 'Noto Mono']\n",
      "['DejaVu Sans Mono', 'DejaVu Serif', 'STIXSizeFourSym', 'STIXNonUnicode', 'STIXSizeThreeSym', 'DejaVu Serif', 'cmss10', 'STIXNonUnicode', 'STIXSizeThreeSym', 'STIXSizeTwoSym', 'STIXNonUnicode', 'cmsy10', 'STIXGeneral', 'DejaVu Sans', 'cmmi10', 'DejaVu Sans Mono', 'DejaVu Sans Display', 'STIXSizeFourSym', 'DejaVu Sans Mono', 'STIXGeneral', 'DejaVu Sans', 'DejaVu Serif', 'DejaVu Serif', 'cmr10', 'STIXNonUnicode', 'STIXGeneral', 'cmb10', 'Times New Roman', 'DejaVu Sans Mono', 'STIXSizeOneSym', 'STIXSizeOneSym', 'STIXGeneral', 'cmex10', 'DejaVu Serif Display', 'STIXSizeTwoSym', 'DejaVu Sans', 'cmtt10', 'DejaVu Sans', 'STIXSizeFiveSym', 'DejaVu Sans', 'DejaVu Serif', 'DejaVu Sans', 'DejaVu Sans Mono', 'Times New Roman', 'DejaVu Sans Mono', 'DejaVu Serif']\n",
      "True\n",
      "/home/mila/y/younesse.kaddar/.config/matplotlib\n",
      "/home/mila/y/younesse.kaddar/.cache/matplotlib\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# # Remove the matplotlib cache\n",
    "# shutil.rmtree(matplotlib.get_cachedir())\n",
    "\n",
    "fonts = matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext=\"ttf\")\n",
    "\n",
    "# print the names of all fonts\n",
    "font_names = [matplotlib.font_manager.get_font(x).family_name for x in fonts]\n",
    "print(font_names)\n",
    "\n",
    "fonts = [f.name for f in matplotlib.font_manager.fontManager.ttflist]\n",
    "print(fonts)\n",
    "print(\"Times New Roman\" in fonts)\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "matplotlib.rc(\"font\", family=\"Times New Roman\")\n",
    "\n",
    "print(matplotlib.get_configdir())\n",
    "print(matplotlib.get_cachedir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 11 06:27:50 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000000:A3:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              44W / 300W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "with torch.no_grad():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the pretrained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/y/younesse.kaddar/virtualenvs/lm_3.10/lib/python3.10/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/mila/y/younesse.kaddar/virtualenvs/lm_3.10/lib/python3.10/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "/home/mila/y/younesse.kaddar/virtualenvs/lm_3.10/lib/python3.10/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Hydra\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"multiobjective-lm/rng/configs\")\n",
    "cfg = compose(config_name=\"config\")\n",
    "\n",
    "warmup_steps = cfg.hparams.warmup_steps\n",
    "max_len = cfg.hparams.max_len\n",
    "min_len = cfg.hparams.min_len\n",
    "eval_interval = cfg.hparams.eval_interval\n",
    "log_interval = cfg.hparams.log_interval\n",
    "model_to_use = cfg.hparams.model_to_use\n",
    "seed = cfg.hparams.seed\n",
    "save_dir = cfg.hparams.save_dir\n",
    "lr = cfg.hparams.PPO.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "set_seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA, Optimizer, PPO Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "current_device = Accelerator().local_process_index\n",
    "print(current_device)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=cfg.lora.r,\n",
    "    lora_alpha=cfg.lora.lora_alpha,\n",
    "    target_modules=[\"k_proj\", \"v_proj\"] if model_to_use == \"gpt-j\" else [\"c_attn\"],\n",
    "    lora_dropout=cfg.lora.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "inference_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    \"nlpcloud/instruct-gpt-j-fp16\" if model_to_use == \"gpt-j\" else \"gpt2\",\n",
    "    # load_in_8bit=True,\n",
    "    device_map={\"\": current_device},\n",
    "    peft_config=lora_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"nlpcloud/instruct-gpt-j-fp16\" if model_to_use == \"gpt-j\" else \"gpt2\"\n",
    ")\n",
    "\n",
    "opt = torch.optim.AdamW(\n",
    "    [{\"params\": inference_model.parameters(), \"lr\": lr}],\n",
    "    betas=(cfg.adamw.b1, cfg.adamw.b2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: No names found, cannot describe anything.\n"
     ]
    }
   ],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=model_to_use,\n",
    "    log_with=\"wandb\",\n",
    "    steps=cfg.hparams.PPO.steps,\n",
    "    learning_rate=cfg.hparams.PPO.learning_rate,\n",
    "    batch_size=cfg.hparams.PPO.batch_size,\n",
    "    ppo_epochs=cfg.hparams.PPO.ppo_epochs,\n",
    "    gradient_accumulation_steps=cfg.hparams.PPO.gradient_accumulation_steps,\n",
    "    early_stopping=cfg.hparams.PPO.early_stopping,\n",
    "    target_kl=cfg.hparams.PPO.target_kl,\n",
    "    init_kl_coef=cfg.hparams.PPO.init_kl_coef,\n",
    "    adap_kl_ctrl=cfg.hparams.PPO.adap_kl_ctrl,\n",
    "    mini_batch_size=cfg.hparams.PPO.mini_batch_size,\n",
    "    optimize_cuda_cache=cfg.hparams.PPO.optimize_cuda_cache,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "# WARNING!! To avoid the bug: https://github.com/huggingface/trl/issues/648\n",
    "# make sure that the following assertion is true!\n",
    "assert config.mini_batch_size * config.gradient_accumulation_steps <= config.batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...: 2048it [00:00, 5510.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from rng.rng_dataset import get_tensors_from_dataframe\n",
    "\n",
    "df_train = pd.read_csv(cfg.file_name.train)\n",
    "\n",
    "input_ids, target_ids = get_tensors_from_dataframe(df_train, tokenizer, method=\"PPO\")\n",
    "train_dataset = TensorDataset(input_ids, target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = len(train_dataset) * cfg.hparams.PPO.ppo_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myoukad\u001b[0m (\u001b[33mox\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mila/y/younesse.kaddar/gfn-lm-tuning/wandb/run-20240111_062914-qvj410il</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ox/trl/runs/qvj410il' target=\"_blank\">resilient-sun-18</a></strong> to <a href='https://wandb.ai/ox/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ox/trl' target=\"_blank\">https://wandb.ai/ox/trl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ox/trl/runs/qvj410il' target=\"_blank\">https://wandb.ai/ox/trl/runs/qvj410il</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rng.rng_utils import number_from_generated_text\n",
    "\n",
    "ref_model = create_reference_model(inference_model, num_shared_layers=20)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "eos_string = tokenizer.decode(tokenizer.eos_token_id)\n",
    "\n",
    "\n",
    "def collator(batch):\n",
    "    input_ids, target_ids = zip(*batch)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"query\": [s.replace(eos_string, \"\") for s in tokenizer.batch_decode(input_ids)],\n",
    "        \"target_ids\": target_ids,\n",
    "    }\n",
    "\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    inference_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=train_dataset,\n",
    "    data_collator=collator,\n",
    "    optimizer=opt,\n",
    "    # lr_scheduler=sched,\n",
    ")\n",
    "\n",
    "generation_kwargs = {\n",
    "    # \"min_length\": -1,\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "output_min_length = 2\n",
    "output_max_length = 128\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "model_save_path = os.path.join(save_dir, \"ppo_model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/256 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [38:16<00:00,  8.97s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch, batch in tqdm(\n",
    "    enumerate(ppo_trainer.dataloader), total=len(ppo_trainer.dataloader)\n",
    "):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "    target_tensors = batch[\"target_ids\"]\n",
    "\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[query.shape[0] :])\n",
    "\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    # Compute reward\n",
    "    rewards = []\n",
    "    # mean_rewards = 0\n",
    "    for response, target in zip(batch[\"response\"], target_tensors):\n",
    "        if len(response) == 0:\n",
    "            # print(\"âŒ Empty response\")\n",
    "            scalar_reward = -16.0\n",
    "        else:\n",
    "            try:\n",
    "                generated_number = int(response.replace(eos_string, \"\").rstrip())\n",
    "                # print(\"âœ… Generated number:\", generated_number)\n",
    "                scalar_reward = 16.0 if 0 <= generated_number <= 100 else -8.0\n",
    "            except:\n",
    "                # print(\"âŒ Error decoding response:\", response)\n",
    "                scalar_reward = -8.0 * len(response)\n",
    "        rewards.append(torch.tensor(scalar_reward).cuda())\n",
    "        # mean_rewards += scalar_reward\n",
    "    # mean_rewards /= len(batch[\"response\"])\n",
    "\n",
    "    # Run PPO step\n",
    "    stats = ppo_trainer.step(list(query_tensors), response_tensors, rewards)\n",
    "    # Average reward\n",
    "    # print(\"ðŸ”¥ Average reward: \", mean_rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    # # Save model every 2 epochs\n",
    "    # if epoch % 2 == 0:\n",
    "    #     if ppo_trainer.accelerator.is_main_process:\n",
    "    #         ppo_trainer.save_pretrained(model_save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test generations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' the', ' number', ' is', ' 113', '.', '\\n', '<|endoftext|>']\n",
      "[' 4', '\\n', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "inference_model.eval()\n",
    "with torch.inference_mode():\n",
    "    prompt_test = \"Randomly generate (uniformly) one single random integer between 0 and 520, and then stop: \"\n",
    "    print(\n",
    "        [\n",
    "            tokenizer.decode(t)\n",
    "            for t in inference_model.generate(\n",
    "                **tokenizer(prompt_test, return_tensors=\"pt\").to(\"cuda\"),\n",
    "                max_new_tokens=30,\n",
    "                temperature=0,\n",
    "            )[0][len(tokenizer.encode(prompt_test)) :]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt_test = (\n",
    "        \"Here is one single random integer sampled uniformly between 0 and 520: \"\n",
    "    )\n",
    "    print(\n",
    "        [\n",
    "            tokenizer.decode(t)\n",
    "            for t in inference_model.generate(\n",
    "                **tokenizer(prompt_test, return_tensors=\"pt\").to(\"cuda\"),\n",
    "                max_new_tokens=30,\n",
    "                temperature=0,\n",
    "            )[0][len(tokenizer.encode(prompt_test)) :]\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_hparams = f\"{model_to_use}_batch_size_{cfg.hparams.PPO.batch_size}_mini_batch_size_{cfg.hparams.PPO.mini_batch_size}_steps_{cfg.hparams.PPO.steps}_learning_rate_{cfg.hparams.PPO.learning_rate}_ppo_epochs_{cfg.hparams.PPO.ppo_epochs}_gradient_accumulation_steps_{cfg.hparams.PPO.gradient_accumulation_steps}_target_kl_{cfg.hparams.PPO.target_kl}_init_kl_coef_{cfg.hparams.PPO.init_kl_coef}_seed_{seed}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created checkpoint ckpts/rng-PPO_gpt-j_batch_size_8_mini_batch_size_1_steps_512_learning_rate_1.41e-05_ppo_epochs_1_gradient_accumulation_steps_8_target_kl_0.1_init_kl_coef_0.2_seed_42\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    print(f\"Creating directory {save_dir}\")\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "ckpt_name = f\"rng-PPO_{suffix_hparams}\"\n",
    "\n",
    "inference_model.save_pretrained(f\"{save_dir}/{ckpt_name}\")\n",
    "\n",
    "print(f\"Created checkpoint {save_dir}/{ckpt_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_name = f\"rng-PPO_{suffix_hparams}\"\n",
    "# model_path = f\"{save_dir}/{ckpt_name}\"\n",
    "# inference_model = PeftModel.from_pretrained(model_to_use, model_path)\n",
    "\n",
    "# print(f\"Saved model to {save_dir}/{ckpt_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rng.rng_utils import get_distribution\n",
    "\n",
    "n_max = 100\n",
    "intro_prompt = f\"The following is a random integer drawn uniformly between 0 and \"\n",
    "prompt = f\"{intro_prompt}{n_max}: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rng.rng_plot import plot_distribution\n",
    "\n",
    "n_samples = 100 * 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [02:41<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "inference_model.eval()\n",
    "# inference_model.base_model.enable_adapter_layers()\n",
    "\n",
    "dist_inference, number_of_NaNs_inference = get_distribution(\n",
    "    inference_model, tokenizer, prompt, num_samples=n_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.hparams.PPO.ppo_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51068 numbers, 132 NaNs\n"
     ]
    }
   ],
   "source": [
    "number_of_numbers = len(dist_inference)\n",
    "\n",
    "print(f\"{number_of_numbers} numbers, {number_of_NaNs_inference} NaNs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## PPO-finetuned Model: Distribution of generated numbers"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_distribution(\n",
    "    dist_inference,\n",
    "    n_max=n_max,\n",
    "    model_name=\"PPO-finetuned Model\",\n",
    "    color=\"goldenrod\",\n",
    "    number_of_NaNs=number_of_NaNs_inference,\n",
    "    xlims=(-5, n_max + 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"plots\"):\n",
    "    os.makedirs(\"plots\")\n",
    "\n",
    "df_inference = pd.DataFrame(dist_inference, columns=[\"Generated Numbers\"])\n",
    "df_inference.to_csv(\n",
    "    f\"plots/PPO_nb-numbers_{number_of_numbers}_nb-NaNs_{number_of_NaNs_inference}_{suffix_hparams}.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plots/PPO_nb-numbers_51068_nb-NaNs_132_gpt-j_batch_size_8_mini_batch_size_1_steps_512_learning_rate_1.41e-05_ppo_epochs_1_gradient_accumulation_steps_8_target_kl_0.1_init_kl_coef_0.2_seed_42.csv\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"plots/PPO_nb-numbers_{number_of_numbers}_nb-NaNs_{number_of_NaNs_inference}_{suffix_hparams}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generated Numbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Generated Numbers\n",
       "0                 49\n",
       "1                 50\n",
       "2                 49\n",
       "3                 42\n",
       "4                 88"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inference.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
